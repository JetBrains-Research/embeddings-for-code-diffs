{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torchtext import data\n",
    "from neural_editor.seq2seq.train import load_data\n",
    "from neural_editor.seq2seq.config import load_config\n",
    "from neural_editor.seq2seq.train_utils import greedy_decode, remove_eos, lookup_words, calculate_accuracy\n",
    "from neural_editor.seq2seq.datasets.dataset_utils import take_part_from_dataset\n",
    "from neural_editor.seq2seq.train_utils import rebatch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_ROOT = '/home/mikhail/Documents/Development/embeddings-for-code-diffs-data/experiment_20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = load_config(False, Path(os.path.join(RESULTS_ROOT, 'config.pkl')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes (number of sentence pairs):\n",
      "train 8793\n",
      "valid 1100\n",
      "test 1098 \n",
      "\n",
      "First training example:\n",
      "src: public void METHOD_1 ( ) { TYPE_1 VAR_1 = new TYPE_1 ( ) ; byte [ ] VAR_2 = TYPE_2 . METHOD_2 ( ) ; VAR_1 . METHOD_3 ( VAR_2 , 0 , VAR_2 . length ) ; org.junit.Assert.assertEquals ( STRING_1 , VAR_1 . METHOD_4 ( ) . get ( STRING_2 ) ) ; }\n",
      "trg: public void METHOD_1 ( ) { TYPE_1 VAR_1 = new TYPE_1 ( ) ; byte [ ] VAR_2 = TYPE_2 . METHOD_2 ( ) ; VAR_1 . METHOD_3 ( VAR_2 , 0 , VAR_2 . length ) ; assertEquals ( STRING_1 , VAR_1 . METHOD_4 ( ) . get ( STRING_2 ) ) ; }\n",
      "diff_alignment: замена\n",
      "diff_prev: org.junit.Assert.assertEquals\n",
      "diff_updated: assertEquals \n",
      "\n",
      "Most common words:\n",
      "         )     194026\n",
      "         (     193976\n",
      "         .      92332\n",
      "         ;      82530\n",
      "   паддинг      54790\n",
      "         ,      54180\n",
      "         {      42036\n",
      "         }      41868\n",
      "     VAR_1      41750\n",
      "  удаление      41184 \n",
      "\n",
      "First 10 words:\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 )\n",
      "05 (\n",
      "06 .\n",
      "07 ;\n",
      "08 паддинг\n",
      "09 , \n",
      "\n",
      "Special words frequency and ids: \n",
      "<unk> 0 0\n",
      "<pad> 0 1\n",
      "<s> 0 2\n",
      "</s> 0 3\n",
      "замена 8514 35\n",
      "удаление 41184 13\n",
      "добавление 13606 23\n",
      "равенство 0 0\n",
      "паддинг 54790 8\n",
      "Number of words (types): 758\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset, diffs_field = load_data(verbose=True, config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_INDEX = diffs_field.vocab.stoi[CONFIG['PAD_TOKEN']]\n",
    "SOS_INDEX = diffs_field.vocab.stoi[CONFIG['SOS_TOKEN']]\n",
    "EOS_INDEX = diffs_field.vocab.stoi[CONFIG['EOS_TOKEN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG._CONFIG['DEVICE'] = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = torch.load(os.path.join(RESULTS_ROOT, 'model_best_on_validation.pt'), map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (generator): Generator(\n",
       "      (projection): Linear(in_features=256, out_features=758, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(758, 128)\n",
       "    (attention): BahdanauAttention(\n",
       "      (key_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (query_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (energy_layer): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "    (rnn): LSTM(416, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
       "    (bridge): Linear(in_features=288, out_features=256, bias=True)\n",
       "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "    (pre_output_layer): Linear(in_features=640, out_features=256, bias=False)\n",
       "  )\n",
       "  (edit_encoder): EditEncoder(\n",
       "    (rnn): LSTM(384, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (embed): Embedding(758, 128)\n",
       "  (generator): Generator(\n",
       "    (projection): Linear(in_features=256, out_features=758, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATASET_300 = take_part_from_dataset(test_dataset, n=300)\n",
    "TEST_DATASET_10 = take_part_from_dataset(test_dataset, n=10)\n",
    "TEST_DATASET_3 = take_part_from_dataset(test_dataset, n=3)\n",
    "TEST_DATASET_2 = take_part_from_dataset(test_dataset, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_with_batch(dataset, batch_size):\n",
    "    iterator = data.Iterator(dataset, batch_size=batch_size, train=False,\n",
    "                                      sort_within_batch=True,\n",
    "                                      sort=False,\n",
    "                                      sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                                      repeat=False,\n",
    "                                      device=CONFIG['DEVICE'])\n",
    "    return calculate_accuracy((rebatch(PAD_INDEX, t, CONFIG) for t in iterator),\n",
    "                                  MODEL,\n",
    "                                  CONFIG['TOKENS_CODE_CHUNK_MAX_LEN'],\n",
    "                                  diffs_field.vocab, CONFIG)\n",
    "\n",
    "def show_calculate_accruacy_diff(dataset):\n",
    "    batch_accuracy = get_accuracy_with_batch(dataset, batch_size=len(dataset))\n",
    "    single_accuracy = get_accuracy_with_batch(dataset, batch_size=1)\n",
    "    print(f'Batch accuracy: {batch_accuracy}')\n",
    "    print(f'Single accuracy: {single_accuracy}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 0.07666666666666666\n",
      "Single accuracy: 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "show_calculate_accruacy_diff(TEST_DATASET_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_decoded(dataset, batch_size):\n",
    "    iterator = data.Iterator(dataset, batch_size=batch_size, train=False,\n",
    "                              sort_within_batch=True,\n",
    "                              sort=False,\n",
    "                              sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
    "                              device=DEVICE)\n",
    "    decoded = []\n",
    "    for batch in iterator:\n",
    "        batch = rebatch(PAD_INDEX, batch, CONFIG)\n",
    "        decoded += greedy_decode(MODEL, batch, CONFIG['TOKENS_CODE_CHUNK_MAX_LEN'], SOS_INDEX, EOS_INDEX)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_from_decoded(dataset):\n",
    "    batch_decoded = np.array(get_batch_decoded(dataset, len(dataset)))\n",
    "    single_decoded = get_batch_decoded(dataset, 1)\n",
    "    sort_keys = [(len(x.src), len(x.trg)) for x in dataset]\n",
    "    sorted_indices = list(reversed([i[0] for i in sorted(enumerate(sort_keys), key=lambda x:x[1])]))\n",
    "    correct_order = [0 for _ in range(len(sorted_indices))]\n",
    "    for i, value in enumerate(sorted_indices):\n",
    "        correct_order[value] = i\n",
    "    batch_decoded_correct_order = list(batch_decoded[correct_order])\n",
    "    \n",
    "    incorrect = []\n",
    "    for i, values in enumerate(zip(batch_decoded_correct_order, single_decoded)):\n",
    "        #print(values)\n",
    "        batch_decoded_value, single_decoded_value = values\n",
    "        if len(batch_decoded_value) != len(single_decoded_value) or \\\n",
    "           (batch_decoded_value != single_decoded_value).any():\n",
    "            incorrect.append((i, batch_decoded_value, single_decoded_value))\n",
    "    return incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_diff_from_decoded(dataset, until=None):\n",
    "    diff = get_diff_from_decoded(dataset)\n",
    "    if len(diff) == 0:\n",
    "        print('NO DIFF')\n",
    "        return\n",
    "    for take_id in range(len(diff[:until])):\n",
    "        print(f'First i = {diff[take_id][0]}')\n",
    "        batch_version = \" \".join(lookup_words(diff[take_id][1], diffs_field.vocab))\n",
    "        single_version = \" \".join(lookup_words(diff[take_id][2], diffs_field.vocab))\n",
    "        target_version = \" \".join(dataset[diff[take_id][0]].trg)\n",
    "        print(f'Batch  : {batch_version}')\n",
    "        print(f'Single : {single_version}')\n",
    "        print(f'Target : {target_version}')\n",
    "        if batch_version == target_version:\n",
    "            print(f'Batch correct')\n",
    "        elif single_version == target_version:\n",
    "            print(f'Single correct')\n",
    "        else:\n",
    "            print(f'None is correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO DIFF\n"
     ]
    }
   ],
   "source": [
    "print_diff_from_decoded(TEST_DATASET_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First i = 2\n",
      "Batch  : public java.lang.String METHOD_1 ( TYPE_1 locale ) { if ( VAR_1 . METHOD_2 ( ) ) { return STRING_1 ; } java.lang.String result = TYPE_2 . METHOD_3 ( METHOD_4 ( ) , locale . METHOD_4 ( ) ) ; if ( result == null ) { result = TYPE_2 . METHOD_3 ( METHOD_4 ( ) , TYPE_1 . METHOD_5 ( ) . METHOD_4 ( ) ) ; } return result ; }\n",
      "Single : public java.lang.String METHOD_1 ( TYPE_1 locale ) { if ( VAR_1 . METHOD_2 ( ) ) { return STRING_1 ; } java.lang.String result = TYPE_2 . METHOD_3 ( METHOD_4 ( ) , locale . METHOD_4 ( ) ) ; if ( result == null ) { result = TYPE_2 . METHOD_3 ( METHOD_4 ( ) . METHOD_4 ( ) ) ; } return result ; }\n",
      "Target : public java.lang.String METHOD_1 ( TYPE_1 locale ) { if ( VAR_1 . METHOD_2 ( ) ) { return STRING_1 ; } java.lang.String result = TYPE_2 . METHOD_1 ( this , locale ) ; if ( result == null ) { result = TYPE_2 . METHOD_1 ( this , TYPE_1 . METHOD_5 ( ) ) ; } return result ; }\n",
      "None is correct\n"
     ]
    }
   ],
   "source": [
    "print_diff_from_decoded(TEST_DATASET_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TO_REPRODUCE_BUG = data.Dataset(test_dataset[1:3], test_dataset.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First i = 1\n",
      "Batch  : public java.lang.String METHOD_1 ( TYPE_1 locale ) { if ( VAR_1 . METHOD_2 ( ) ) { return STRING_1 ; } java.lang.String result = TYPE_2 . METHOD_3 ( METHOD_4 ( ) , locale . METHOD_4 ( ) ) ; if ( result == null ) { result = TYPE_2 . METHOD_3 ( METHOD_4 ( ) , TYPE_1 . METHOD_5 ( ) . METHOD_4 ( ) ) ; } return result ; }\n",
      "Single : public java.lang.String METHOD_1 ( TYPE_1 locale ) { if ( VAR_1 . METHOD_2 ( ) ) { return STRING_1 ; } java.lang.String result = TYPE_2 . METHOD_3 ( METHOD_4 ( ) , locale . METHOD_4 ( ) ) ; if ( result == null ) { result = TYPE_2 . METHOD_3 ( METHOD_4 ( ) . METHOD_4 ( ) ) ; } return result ; }\n",
      "Target : public java.lang.String METHOD_1 ( TYPE_1 locale ) { if ( VAR_1 . METHOD_2 ( ) ) { return STRING_1 ; } java.lang.String result = TYPE_2 . METHOD_1 ( this , locale ) ; if ( result == null ) { result = TYPE_2 . METHOD_1 ( this , TYPE_1 . METHOD_5 ( ) ) ; } return result ; }\n",
      "None is correct\n"
     ]
    }
   ],
   "source": [
    "print_diff_from_decoded(DATASET_TO_REPRODUCE_BUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"BUGGY VERSION\"\n",
    "def greedy_decode(model, batch,\n",
    "                  max_len: int,\n",
    "                  sos_index: int, eos_index: int):\n",
    "    \"\"\"\n",
    "    Greedily decode a sentence.\n",
    "    :return: [DecodedSeqLenCutWithEos]\n",
    "    \"\"\"\n",
    "    # TODO: create beam search\n",
    "    # [B, SrcSeqLen], [B, 1, SrcSeqLen], [B]\n",
    "    src, src_mask, src_lengths = batch.src, batch.src_mask, batch.src_lengths\n",
    "    with torch.no_grad():\n",
    "        edit_final, encoder_output, encoder_final = model.encode(batch)\n",
    "        prev_y = torch.ones(batch.nseqs, 1).fill_(sos_index).type_as(src)  # [B, 1]\n",
    "        trg_mask = torch.ones_like(prev_y)  # [B, 1]\n",
    "\n",
    "    output = torch.zeros((batch.nseqs, max_len))\n",
    "    states = None\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            # pre_output: [B, TrgSeqLen, DecoderH]\n",
    "            out, states, pre_output = model.decode(edit_final, encoder_output, encoder_final,\n",
    "                                                   src_mask, prev_y, trg_mask, states)\n",
    "\n",
    "            # we predict from the pre-output layer, which is\n",
    "            # a combination of Decoder state, prev emb, and context\n",
    "            prob = model.generator(pre_output[:, -1])  # [B, V]\n",
    "\n",
    "        _, next_words = torch.max(prob, dim=1)\n",
    "        output[:, i] = next_words\n",
    "        prev_y[:, 0] = next_words\n",
    "\n",
    "    output = output.cpu().long().numpy()\n",
    "    return remove_eos(output, eos_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO DIFF\n"
     ]
    }
   ],
   "source": [
    "print_diff_from_decoded(DATASET_TO_REPRODUCE_BUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like we fixed a bug. I forgot to permute results back after sorting for pack padded sequence in EditEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch accuracy: 0.08333333333333333\n",
      "Single accuracy: 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "show_calculate_accruacy_diff(TEST_DATASET_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_DECODED_2 = get_batch_decoded(TEST_DATASET_2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_DECODED_2 = get_batch_decoded(TEST_DATASET_2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhail/anaconda3/envs/embeddings-for-code-diffs/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-4a5c33dde01e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_DECODED_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_DECODED_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
