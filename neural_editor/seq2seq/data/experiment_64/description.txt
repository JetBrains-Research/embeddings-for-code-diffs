Same as 60 but neural editor is trained.

Observations:
64 vs 60
Loss plot looks pretty the same as in 60.

neural editor (compared with 63 when dataset unfiltered + unfiltered)
best_val_perplexity: 10.64 vs 6.93 (worse)
last_train_perplexity: 1.39 vs 1.45 (a little bit better)
test_acc: 0.105 vs 0.123 (worse)
train_acc_test_size: 0.443 vs 0.388 (much better)
train_acc_test_size top-50: 0.678 vs 0.657 (better)
BLEU test: 70.47 vs 70.32 (a little bit better)
BLEU train: 89.27 vs 88.52 (better)
hyp_len / ref_len (on test): 0.908 vs 0.853 (better)

commit message generator
best_val_perplexity: 89.21 vs 90.81 (a little bit better)
last_train_perplexity: 12.02 vs 13.56 (a little bit better)
test_acc: 0.05 vs 0.046 (better)
train_acc_test_size: 0.067 vs 0.057 (better)
train_acc_test_size top-50: 0.193 vs 0.186 (better)
BLEU test: 20.21 vs 19.61 (better)
BLEU train: 20.39 vs 19.13 (better)
hyp_len / ref_len (on test): 0.483 vs 0.476 (better)

Conclusions: Interestingly but neural editor is trained in terms of perplexity and test_acc worse when data is reduced twice but in terms of other metrics including BLEU it is a little bit better. Again this can be because task is self-sustained. I think in this case "a little bit better" means the same performance.
As we understood for cmg task the performance is very dependent on the data and amount of it. Therefore we compare with the same data. And results show that performance is a little bit better. Then maybe edit representation vectors help for true.
