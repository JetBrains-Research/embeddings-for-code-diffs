Same as 26 but LEAVE_ONLY_CHANGED tokens False and edit representation dim 512.

Observations: best val perplexity better than in 26 (1.16 vs 1.28), top-1 accuracy on test is better (0.6 vs 0.45), BUT transfer learning performs very poorly (same edit representation 0.89 vs 0.88, but for other 0.05 vs 0.35) => dim 512 for edits is very huge, even for LEAVE_ONLY_CHANGED tokens = False.
