{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of nearest neighbours performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset, Field\n",
    "from torchtext import data\n",
    "from datasets.CommitMessageGenerationDataset import CommitMessageGenerationDataset\n",
    "from datasets.CodeChangesDataset import CodeChangesTokensDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import torch\n",
    "import os\n",
    "from neural_editor.seq2seq.train_utils import rebatch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from neural_editor.seq2seq.experiments.BleuCalculation import BleuCalculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'TOKEN_MIN_FREQ': 1,\n",
    "        'TOKENS_CODE_CHUNK_MAX_LEN': 121,\n",
    "        'MSG_MAX_LEN': 30,\n",
    "        'LOWER': True,\n",
    "        'LOWER_COMMIT_MSG': True,\n",
    "        'UNK_TOKEN': \"<unk>\",\n",
    "        'PAD_TOKEN': \"<pad>\",\n",
    "        'SOS_TOKEN': \"<s>\",\n",
    "        'EOS_TOKEN': \"</s>\",\n",
    "        'REPLACEMENT_TOKEN': 'замена',\n",
    "        'DELETION_TOKEN': 'удаление',\n",
    "        'ADDITION_TOKEN': 'добавление',\n",
    "        'UNCHANGED_TOKEN': 'равенство',\n",
    "        'PADDING_TOKEN': 'паддинг',\n",
    "        'LEAVE_ONLY_CHANGED': True,\n",
    "        'DEVICE': torch.device('cpu'),\n",
    "        'BLEU_PERL_SCRIPT_PATH': '/home/mikhail/Documents/Development/embeddings-for-code-diffs/neural_editor/seq2seq/experiments/multi-bleu.perl'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path: str, dataset_path_commit: str):\n",
    "    config['DATASET_ROOT'] = dataset_path\n",
    "    config['DATASET_ROOT_COMMIT'] = dataset_path_commit\n",
    "        \n",
    "    train_dataset, val_dataset, test_dataset, diffs_field = \\\n",
    "        CodeChangesTokensDataset.load_data(True, config)\n",
    "    train_dataset_commit, val_dataset_commit, test_dataset_commit, fields_commit = \\\n",
    "        CommitMessageGenerationDataset.load_data(diffs_field, True, config)\n",
    "    \n",
    "    data = {'train': (train_dataset, train_dataset_commit), \n",
    "            'val': (val_dataset, val_dataset_commit), \n",
    "            'test': (test_dataset, test_dataset_commit)\n",
    "           }\n",
    "    messages = {'train': [], \n",
    "                'val': [], \n",
    "                'test': []\n",
    "               }\n",
    "    for path in [dataset_path, dataset_path_commit]:\n",
    "        for mode in ['train', 'val', 'test']:\n",
    "            with open(os.path.join(path, mode, 'msg.txt'), mode='r', encoding='utf-8') as msg:\n",
    "                messages[mode].append([l.strip() for l in msg])\n",
    "    return data, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes (number of sentence pairs):\n",
      "train 10402\n",
      "valid 1214\n",
      "test 1160 \n",
      "\n",
      "Max sequence length in tokens: 94 \n",
      "\n",
      "First training example:\n",
      "src: mmm a / changelog . md <nl> * storm - 464 : simulated time advanced after test cluster exits causes intermittent test failures <nl> * storm - 463 : added static version of metrics helpers for config <nl> * storm - 376 : add compression to serialization <nl> # # 0 . 9 . 2 - incubating <nl> * storm - 66 : send taskid on initial handshake <nl>\n",
      "trg: ppp b / changelog . md <nl> * storm - 464 : simulated time advanced after test cluster exits causes intermittent test failures <nl> * storm - 463 : added static version of metrics helpers for config <nl> * storm - 376 : add compression to serialization <nl> * storm - 437 : enforce utf - 8 when multilang reads from stdin <nl> # # 0 . 9 . 2 - incubating <nl> * storm - 66 : send taskid on initial handshake <nl>\n",
      "diff_alignment: замена замена добавление добавление добавление добавление добавление добавление добавление добавление добавление добавление добавление добавление добавление добавление добавление\n",
      "diff_prev: mmm a паддинг паддинг паддинг паддинг паддинг паддинг паддинг паддинг паддинг паддинг паддинг паддинг паддинг паддинг паддинг\n",
      "diff_updated: ppp b <nl> * storm - 437 : enforce utf - 8 when multilang reads from stdin \n",
      "\n",
      "Most common words:\n",
      "      <nl>     157824\n",
      "         /     107466\n",
      "         .      76478\n",
      "   паддинг      65112\n",
      "добавление      45630\n",
      "         =      37056\n",
      "         -      35334\n",
      "    замена      31876\n",
      "         a      22090\n",
      "         b      20962 \n",
      "\n",
      "First 10 words:\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 <nl>\n",
      "05 /\n",
      "06 .\n",
      "07 паддинг\n",
      "08 добавление\n",
      "09 = \n",
      "\n",
      "Special words frequency and ids: \n",
      "<unk> 0 0\n",
      "<pad> 0 1\n",
      "<s> 0 2\n",
      "</s> 0 3\n",
      "замена 31876 11\n",
      "удаление 19482 17\n",
      "добавление 45630 8\n",
      "равенство 0 0\n",
      "паддинг 65112 7\n",
      "Number of words (types): 33711\n",
      "Data set sizes (number of sentence pairs):\n",
      "train 10403\n",
      "valid 1215\n",
      "test 1161 \n",
      "\n",
      "Max src sequence length in tokens: 89\n",
      "Max trg sequence length in tokens: 30\n",
      "Max diff sequence length in tokens: 68 \n",
      "\n",
      "First training example:\n",
      "src: mmm a / modules / apps / foundation / portal - scheduler / . gitrepo <nl> ; <nl> [ subrepo ] <nl> cmdver = liferay <nl> commit = 7495702b9e0c1ed26cd9e99a029e63ea34463245 <nl> mode = push <nl> parent = e48ff9193f8932fc19647e7f7b18fce5d898c8e1 <nl> remote = git @ github . com : liferay / com - liferay - portal - scheduler . git <nl> \\ no newline at end of file <nl>\n",
      "trg: ignore update ' modules / apps / foundation / portal - scheduler / .\n",
      "diff_alignment: замена замена замена замена\n",
      "diff_prev: mmm a 7495702b9e0c1ed26cd9e99a029e63ea34463245 e48ff9193f8932fc19647e7f7b18fce5d898c8e1\n",
      "diff_updated: ppp b ecfc76fa14a34359ac940f76dbb71388d1de77ad 5a8692e1348dbf702e0a0e19b91dbb0057ef3600\n",
      "Most common words in src vocabulary:\n",
      "      <nl>     157824\n",
      "         /     107466\n",
      "         .      76478\n",
      "   паддинг      65112\n",
      "добавление      45630\n",
      "         =      37056\n",
      "         -      35334\n",
      "    замена      31876\n",
      "         a      22090\n",
      "         b      20962 \n",
      "\n",
      "\n",
      "Most common words in trg vocabulary:\n",
      "         /       6457\n",
      "         .       5843\n",
      "         -       3034\n",
      "        to       2260\n",
      "         '       2081\n",
      "    update       1966\n",
      "    ignore       1845\n",
      "   modules       1588\n",
      "      apps       1579\n",
      "       the       1132 \n",
      "\n",
      "\n",
      "Most common words in diff vocabulary:\n",
      "      <nl>     157824\n",
      "         /     107466\n",
      "         .      76478\n",
      "   паддинг      65112\n",
      "добавление      45630\n",
      "         =      37056\n",
      "         -      35334\n",
      "    замена      31876\n",
      "         a      22090\n",
      "         b      20962 \n",
      "\n",
      "First 10 words in src vocabulary:\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 <nl>\n",
      "05 /\n",
      "06 .\n",
      "07 паддинг\n",
      "08 добавление\n",
      "09 = \n",
      "\n",
      "\n",
      "First 10 words in trg vocabulary:\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 /\n",
      "05 .\n",
      "06 -\n",
      "07 to\n",
      "08 '\n",
      "09 update \n",
      "\n",
      "\n",
      "First 10 words in diff vocabulary:\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <s>\n",
      "03 </s>\n",
      "04 <nl>\n",
      "05 /\n",
      "06 .\n",
      "07 паддинг\n",
      "08 добавление\n",
      "09 = \n",
      "\n",
      "Special words frequency and ids in src vocabulary: \n",
      "<unk> 0 0\n",
      "<pad> 0 1\n",
      "<s> 0 2\n",
      "</s> 0 3\n",
      "Special words frequency and ids in trg vocabulary: \n",
      "<unk> 0 0\n",
      "<pad> 0 1\n",
      "<s> 0 2\n",
      "</s> 0 3\n",
      "Special words frequency and ids in diffs_field vocabulary: \n",
      "<unk> 0 0\n",
      "<pad> 0 1\n",
      "<s> 0 2\n",
      "</s> 0 3\n",
      "замена 31876 11\n",
      "удаление 19482 17\n",
      "добавление 45630 8\n",
      "равенство 0 0\n",
      "паддинг 65112 7\n",
      "Number of words (types) in src vocabulary: 33711\n",
      "Number of words (types) in trg vocabulary: 7689\n",
      "Number of words (types) in diff vocabulary: 33711\n"
     ]
    }
   ],
   "source": [
    "JIANG_FILTERED_PART_DATASET_PATH = '../embeddings-for-code-diffs-data/datasets/commit_message_generation/Jiang/filtered_dataset/partitioned/'\n",
    "JIANG_FILTERED_PART_DATA, JIANG_FILTERED_PART_MESSAGES = load_dataset(JIANG_FILTERED_PART_DATASET_PATH + 'neural_editor', JIANG_FILTERED_PART_DATASET_PATH + 'commit_message_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ne_model_from_experiment(experiment):\n",
    "    path = f'../embeddings-for-code-diffs-data/experiment_{experiment}/model_best_on_validation_neural_editor.pt'\n",
    "    return torch.load(path, map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhail/anaconda3/envs/embeddings-for-code-diffs/lib/python3.6/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/mikhail/anaconda3/envs/embeddings-for-code-diffs/lib/python3.6/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "MODEL_E107 = load_ne_model_from_experiment('107')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, dataset):\n",
    "    from tqdm.auto import tqdm\n",
    "    X = [None] * len(dataset)\n",
    "    pad_index = dataset.fields['src'].vocab.stoi['<pad>']\n",
    "    data_iterator = data.Iterator(dataset, batch_size=64, train=False,\n",
    "                                  shuffle=False,\n",
    "                                  sort=False,\n",
    "                                  sort_within_batch=True,\n",
    "                                  sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                                  device=DEVICE)\n",
    "    data_iterator = [rebatch(pad_index, batch, dataset, config) for batch in data_iterator]\n",
    "    for batch in tqdm(data_iterator):\n",
    "        edit_final, encoder_output, encoder_final = model.encode(batch)\n",
    "        edit_final = torch.cat((edit_final[0], encoder_final[0]), dim=-1)\n",
    "        #edit_final = edit_final[0]\n",
    "        for i, idx in enumerate(batch.ids):\n",
    "            X[idx] = edit_final[-1][i].detach().numpy()\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e984cc40af4d4005b2ff9b6223008fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=163), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_TRAIN = extract_features(MODEL_E107, JIANG_FILTERED_PART_DATA['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_TRAIN = JIANG_FILTERED_PART_MESSAGES['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10402, 288)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEAREST_NEIGHBOUR = NearestNeighbors(n_neighbors=1, algorithm='brute').fit(X_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e9fbd9729147f9a349e216971a9bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_TEST = extract_features(MODEL_E107, JIANG_FILTERED_PART_DATA['test'][1])\n",
    "Y_TEST = JIANG_FILTERED_PART_MESSAGES['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_PRED = [[Y_TRAIN[i[0]].lower().split()] for i in NEAREST_NEIGHBOUR.kneighbors(X_TEST)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start conducting BLEU calculation experiment for NB dataset...\n",
      "b'BLEU = 18.92, 28.8/20.3/17.4/16.7 (BP=0.933, ratio=0.935, hyp_len=21382, ref_len=22871)\\n'\n",
      "Errors: b''\n"
     ]
    }
   ],
   "source": [
    "BleuCalculation(config).conduct(Y_PRED, JIANG_FILTERED_PART_DATA['test'][1], 'NB dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start conducting BLEU calculation experiment for NB dataset E95...\n",
      "b'BLEU = 40.31, 44.8/39.5/39.3/40.8 (BP=0.981, ratio=0.982, hyp_len=9176, ref_len=9348)\\n'\n",
      "Errors: b''\n"
     ]
    }
   ],
   "source": [
    "BleuCalculation(config).conduct(Y_PRED, JIANG_FILTERED_PART_DATA['test'][1], 'NB dataset E95')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = NEAREST_NEIGHBOUR.kneighbors(X_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3059  9485  7133  3974  5570  3065  4722  4536 10080 10309  8941]\n",
      "[['_']]\n",
      "13 [['_']]\n",
      "125 [['_']]\n",
      "208 [['_']]\n",
      "312 [['_']]\n",
      "345 [['_']]\n",
      "502 [['_']]\n",
      "765 [['_']]\n",
      "779 [['_']]\n",
      "781 [['_']]\n",
      "864 [['_']]\n",
      "911 "
     ]
    }
   ],
   "source": [
    "print(NB[1][NB[0] == 0])\n",
    "for i, d in enumerate(NEAREST_NEIGHBOUR.kneighbors(X_TEST)[0]):\n",
    "    if d == 0:\n",
    "        print(Y_PRED[i])\n",
    "        Y_PRED[i] = [['gkldsflkjgdfncvbkjn2314234']]\n",
    "        print(i, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start conducting BLEU calculation experiment for NB dataset E95...\n",
      "b'BLEU = 39.66, 44.4/39.1/38.9/40.5 (BP=0.975, ratio=0.975, hyp_len=9118, ref_len=9348)\\n'\n",
      "Errors: b''\n"
     ]
    }
   ],
   "source": [
    "BleuCalculation(config).conduct(Y_PRED, JIANG_FILTERED_PART_DATA['test'][1], 'NB dataset E95')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_PRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
